{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Credential**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.env')\n",
    "print(env_path)\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "print(\"Environment variables loaded from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r96dThRHUzOE"
   },
   "source": [
    "## **Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ============================================================================\n",
    "## MACHINE LEARNING & DEEP LEARNING TEMPLATE - LIBRARY IMPORTS\n",
    "## ============================================================================\n",
    "## Template ini menyediakan import library lengkap untuk project ML/DL\n",
    "## Uncomment (#) library yang dibutuhkan untuk project Anda\n",
    "## ============================================================================\n",
    "\n",
    "## ============================================================================\n",
    "## =                        Akmaleyzal - 21/08/2023                           =\n",
    "## ============================================================================\n",
    "\n",
    "\n",
    "## ============================================================================\n",
    "## 1. BASIC LIBRARIES - CORE DEPENDENCIES\n",
    "## ============================================================================\n",
    "\n",
    "## Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import polars as pl\n",
    "\n",
    "\n",
    "## Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.offline as pyo\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.io as pio\n",
    "# pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "## System and utilities\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import ( \n",
    "    datetime, \n",
    "    #timedelta\n",
    ")\n",
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple,\n",
    "    Union,\n",
    "    Optional,\n",
    ")\n",
    "# from functools import reduce\n",
    "# from collections import defaultdict, Counter\n",
    "# from itertools import chain, combinations\n",
    "\n",
    "## Progress bars\n",
    "# from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "## Random state management\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## ============================================================================\n",
    "## 2. SCIKIT-LEARN - TRADITIONAL MACHINE LEARNING\n",
    "## ============================================================================\n",
    "\n",
    "## Model Selection and Validation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    # cross_val_score,\n",
    "    # cross_validate,\n",
    "    # GridSearchCV,\n",
    "    # RandomizedSearchCV,\n",
    "    # StratifiedKFold,\n",
    "    # KFold,\n",
    "    # TimeSeriesSplit,\n",
    "    # LeaveOneOut,\n",
    "    # validation_curve,\n",
    "    # learning_curve,\n",
    "    # ShuffleSplit,\n",
    "    # GroupKFold,\n",
    "    # GroupShuffleSplit,\n",
    "    # StratifiedShuffleSplit,\n",
    "    # StratifiedGroupKFold,\n",
    "    # RepeatedStratifiedKFold,\n",
    ")\n",
    "\n",
    "## Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    # RobustScaler,\n",
    "    # MaxAbsScaler,\n",
    "    # Normalizer,\n",
    "    # PowerTransformer,\n",
    "    # QuantileTransformer,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    # OrdinalEncoder,\n",
    "    # TargetEncoder,\n",
    "    # PolynomialFeatures,\n",
    "    # KBinsDiscretizer\n",
    "    # FunctionTransformer,\n",
    "    # Binarizer,\n",
    ")\n",
    "\n",
    "## Feature Selection\n",
    "from sklearn.feature_selection import (\n",
    "        SelectKBest,\n",
    "        # SelectPercentile,\n",
    "        # chi2,\n",
    "        # f_classif,\n",
    "        # f_regression,\n",
    "        # mutual_info_classif,\n",
    "        # mutual_info_regression,\n",
    "        # RFE,\n",
    "        # RFECV,\n",
    "        # SelectFromModel,\n",
    "        # VarianceThreshold\n",
    "        # SequentialFeatureSelector,\n",
    "        # GenericUnivariateSelect,\n",
    "        # f_classif as f_classif_selector,\n",
    "        # f_regression as f_regression_selector,\n",
    "        # mutual_info_classif as mutual_info_classif_selector,\n",
    "        # mutual_info_regression as mutual_info_regression_selector,\n",
    ")\n",
    "\n",
    "## Imputation\n",
    "from sklearn.impute import (\n",
    "#     SimpleImputer,\n",
    "#     KNNImputer,\n",
    "    IterativeImputer.\n",
    ")\n",
    "\n",
    "## Classification Algorithms\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import (\n",
    "#     RandomForestClassifier,\n",
    "#     GradientBoostingClassifier,\n",
    "#     AdaBoostClassifier,\n",
    "#     ExtraTreesClassifier,\n",
    "#     VotingClassifier,\n",
    "#     BaggingClassifier\n",
    "# )\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import (\n",
    "#     GaussianNB,\n",
    "#     MultinomialNB,\n",
    "#     BernoulliNB\n",
    "# )\n",
    "# from sklearn.discriminant_analysis import (\n",
    "#     LinearDiscriminantAnalysis,\n",
    "#     QuadraticDiscriminantAnalysis\n",
    "# )\n",
    "\n",
    "\n",
    "## Regression Algorithms\n",
    "# from sklearn.linear_model import (\n",
    "#     LinearRegression,\n",
    "#     Ridge,\n",
    "#     Lasso,\n",
    "#     ElasticNet,\n",
    "#     SGDRegressor,\n",
    "#     BayesianRidge,\n",
    "#     HuberRegressor\n",
    "# )\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import (\n",
    "#     RandomForestRegressor,\n",
    "#     GradientBoostingRegressor,\n",
    "#     AdaBoostRegressor,\n",
    "#     ExtraTreesRegressor,\n",
    "#     VotingRegressor,\n",
    "#     BaggingRegressor\n",
    "# )\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "## Clustering Algorithms\n",
    "# from sklearn.cluster import (\n",
    "#     KMeans,\n",
    "#     DBSCAN,\n",
    "#     AgglomerativeClustering,\n",
    "#     SpectralClustering,\n",
    "#     MeanShift,\n",
    "#     Birch,\n",
    "#     MiniBatchKMeans\n",
    "# )\n",
    "\n",
    "## Dimensionality Reduction\n",
    "# from sklearn.decomposition import (\n",
    "#     PCA,\n",
    "#     TruncatedSVD,\n",
    "#     FastICA,\n",
    "#     FactorAnalysis,\n",
    "#     NMF,\n",
    "#     LatentDirichletAllocation\n",
    "# )\n",
    "# from sklearn.manifold import (\n",
    "#     TSNE,\n",
    "#     MDS,\n",
    "#     Isomap,\n",
    "#     LocallyLinearEmbedding\n",
    "# )\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "## Metrics and Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    # roc_auc_score,\n",
    "    # roc_curve,\n",
    "    # precision_recall_curve,\n",
    "    # average_precision_score,\n",
    "    # confusion_matrix,\n",
    "    # classification_report,\n",
    "    # mean_squared_error,\n",
    "    # mean_absolute_error,\n",
    "    # r2_score,\n",
    "    # mean_squared_log_error,\n",
    "    # silhouette_score,\n",
    "    # adjusted_rand_score,\n",
    "    # normalized_mutual_info_score\n",
    "    # adjusted_mutual_info_score,\n",
    "    # homogeneity_score,\n",
    "    # completeness_score,\n",
    "    # v_measure_score,\n",
    "    # calinski_harabasz_score,\n",
    "    # davies_bouldin_score,\n",
    "    # explained_variance_score,\n",
    "    # max_error,\n",
    ")\n",
    "\n",
    "## Pipeline\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "## ============================================================================\n",
    "## 3. TENSORFLOW & KERAS - DEEP LEARNING\n",
    "## ============================================================================\n",
    "\n",
    "## TensorFlow Core\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "## Keras Layers\n",
    "# from tensorflow.keras.layers import (\n",
    "#     Dense,\n",
    "#     Conv2D,\n",
    "#     Conv1D,\n",
    "#     MaxPooling2D,\n",
    "#     MaxPooling1D,\n",
    "#     GlobalMaxPooling2D,\n",
    "#     GlobalAveragePooling2D,\n",
    "#     Flatten,\n",
    "#     Dropout,\n",
    "#     BatchNormalization,\n",
    "#     LayerNormalization,\n",
    "#     LSTM,\n",
    "#     GRU,\n",
    "#     SimpleRNN,\n",
    "#     Bidirectional,\n",
    "#     Embedding,\n",
    "#     Input,\n",
    "#     Concatenate,\n",
    "#     Add,\n",
    "#     Multiply,\n",
    "#     Lambda,\n",
    "#     Reshape,\n",
    "#     Permute,\n",
    "#     RepeatVector,\n",
    "#     TimeDistributed,\n",
    "#     Conv2DTranspose,\n",
    "#     UpSampling2D,\n",
    "#     ZeroPadding2D,\n",
    "#     Cropping2D,\n",
    "#     SeparableConv2D,\n",
    "#     DepthwiseConv2D,\n",
    "#     ConvLSTM2D,\n",
    "#     MultiHeadAttention,\n",
    "#     Attention,\n",
    "#     Activation,\n",
    "#     LeakyReLU,\n",
    "#     PReLU,\n",
    "#     ELU,\n",
    "#     ReLU,\n",
    "#     Softmax,\n",
    "#     Swish,\n",
    "#     ThresholdedReLU,\n",
    "#     Softplus,\n",
    "#     Softsign,\n",
    "# )\n",
    "\n",
    "## Keras Models\n",
    "# from tensorflow.keras.models import (\n",
    "#     Sequential,\n",
    "#     Model,\n",
    "#     load_model\n",
    "# )\n",
    "\n",
    "## Keras Optimizers\n",
    "# from tensorflow.keras.optimizers import (\n",
    "#     Adam,\n",
    "#     SGD,\n",
    "#     RMSprop,\n",
    "#     Adagrad,\n",
    "#     Adadelta,\n",
    "#     Adamax,\n",
    "#     Nadam,\n",
    "#     Ftrl,\n",
    "#     Optimizer,\n",
    "#     schedules,\n",
    "#     LearningRateSchedule,\n",
    "# )\n",
    "\n",
    "## Keras Loss Functions\n",
    "# from tensorflow.keras.losses import (\n",
    "#     BinaryCrossentropy,\n",
    "#     CategoricalCrossentropy,\n",
    "#     SparseCategoricalCrossentropy,\n",
    "#     MeanSquaredError,\n",
    "#     MeanAbsoluteError,\n",
    "#     Huber,\n",
    "#     LogCosh,\n",
    "#     Hinge,\n",
    "#     SquaredHinge,\n",
    "#     KLDivergence,\n",
    "#     Poisson,\n",
    "#     CosineSimilarity,\n",
    "#     SparseCategoricalHinge,\n",
    "#     CategoricalHinge,\n",
    "# )\n",
    "\n",
    "## Keras Metrics\n",
    "# from tensorflow.keras.metrics import (\n",
    "#     Accuracy,\n",
    "#     Precision,\n",
    "#     Recall,\n",
    "#     AUC,\n",
    "#     TopKCategoricalAccuracy,\n",
    "#     SparseCategoricalAccuracy,\n",
    "#     MeanSquaredError as MSE_Metric,\n",
    "#     MeanAbsoluteError as MAE_Metric,\n",
    "#     RootMeanSquaredError,\n",
    "# )\n",
    "\n",
    "## Keras Callbacks\n",
    "# from tensorflow.keras.callbacks import (\n",
    "#     EarlyStopping,\n",
    "#     ModelCheckpoint,\n",
    "#     ReduceLROnPlateau,\n",
    "#     TensorBoard,\n",
    "#     CSVLogger,\n",
    "#     LambdaCallback,\n",
    "#     ProgbarLogger,\n",
    "#     RemoteMonitor,\n",
    "#     LearningRateScheduler,\n",
    "#     TerminateOnNaN,\n",
    "#     BackupAndRestore,\n",
    "#     History,\n",
    "#     Callback\n",
    "#     BaseLogger,\n",
    "#     BaseSaver,\n",
    "# )\n",
    "\n",
    "## Keras Regularizers\n",
    "# from tensorflow.keras.regularizers import (\n",
    "#     l1,\n",
    "#     l2,\n",
    "#     L1L2,\n",
    "# )\n",
    "\n",
    "## Keras Preprocessing\n",
    "# from tensorflow.keras.preprocessing.image import (\n",
    "#     ImageDataGenerator,\n",
    "#     load_img,\n",
    "#     img_to_array,\n",
    "#     array_to_img,\n",
    "#     save_img,\n",
    "#     smart_resize,\n",
    "# )\n",
    "# from tensorflow.keras.preprocessing.text import (\n",
    "#     Tokenizer,\n",
    "#     text_to_word_sequence,\n",
    "#     one_hot\n",
    "# )\n",
    "# from tensorflow.keras.preprocessing.sequence import (\n",
    "#     pad_sequences,\n",
    "#     make_sampling_table,\n",
    "#     skipgrams\n",
    "# )\n",
    "\n",
    "## Keras Utils\n",
    "# from tensorflow.keras.utils import (\n",
    "#     to_categorical,\n",
    "#     plot_model,\n",
    "#     model_to_dot,\n",
    "#     normalize,\n",
    "#     get_file,\n",
    "#     Sequence\n",
    "# )\n",
    "\n",
    "## Keras Applications (Pre-trained Models)\n",
    "# from tensorflow.keras.applications import (\n",
    "#     VGG16,\n",
    "#     VGG19,\n",
    "#     ResNet50,\n",
    "#     ResNet101,\n",
    "#     ResNet152,\n",
    "#     InceptionV3,\n",
    "#     InceptionResNetV2,\n",
    "#     MobileNet,\n",
    "#     MobileNetV2,\n",
    "#     DenseNet121,\n",
    "#     DenseNet169,\n",
    "#     DenseNet201,\n",
    "#     NASNetMobile,\n",
    "#     NASNetLarge,\n",
    "#     EfficientNetB0,\n",
    "#     EfficientNetB1,\n",
    "#     EfficientNetB2,\n",
    "#     EfficientNetB3,\n",
    "#     EfficientNetB4,\n",
    "#     EfficientNetB5,\n",
    "#     EfficientNetB6,\n",
    "#     EfficientNetB7\n",
    "# )\n",
    "\n",
    "## TensorFlow Datasets\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "## TensorFlow Probability\n",
    "# import tensorflow_probability as tfp\n",
    "\n",
    "## ============================================================================\n",
    "## 4. PYTORCH - DEEP LEARNING ALTERNATIVE\n",
    "## ============================================================================\n",
    "\n",
    "## PyTorch Core\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import (\n",
    "#     DataLoader,\n",
    "#     Dataset,\n",
    "#     TensorDataset,\n",
    "#     random_split,\n",
    "#     Subset\n",
    "# )\n",
    "\n",
    "## PyTorch Distributed\n",
    "# import torch.distributed as dist\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# import torch.multiprocessing as mp\n",
    "\n",
    "## PyTorch Optimizers\n",
    "# from torch.optim.lr_scheduler import (\n",
    "#     StepLR,\n",
    "#     ReduceLROnPlateau\n",
    "#     CosineAnnealingLR,\n",
    "#     ExponentialLR,\n",
    "#     LambdaLR\n",
    "# )\n",
    "\n",
    "## PyTorch Vision\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision import datasets, models\n",
    "# import torchvision.transforms.functional as TF\n",
    "# from torchvision.datasets import (\n",
    "#     MNIST,\n",
    "#     FashionMNIST,\n",
    "#     CIFAR10,\n",
    "#     CIFAR100,\n",
    "#     ImageFolder,\n",
    "#     DatasetFolder\n",
    "# )\n",
    "\n",
    "## PyTorch Audio\n",
    "# import torchaudio\n",
    "# import torchaudio.transforms as T\n",
    "\n",
    "## PyTorch Text\n",
    "# import torchtext\n",
    "# from torchtext.data import Field, BucketIterator\n",
    "# from torchtext.datasets import (\n",
    "#     IMDB,\n",
    "#     Multi30k,\n",
    "#     SNLI,\n",
    "#     WikiText2,\n",
    "#     WikiText103\n",
    "# )\n",
    "# from torchtext.vocab import Vocab, GloVe, FastText, CharNGram\n",
    "\n",
    "## PyTorch Lightning (High-level PyTorch)\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning import Trainer\n",
    "# from pytorch_lightning.callbacks import (\n",
    "#     EarlyStopping,\n",
    "#     ModelCheckpoint,\n",
    "#     LearningRateMonitor\n",
    "# )\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "## ============================================================================\n",
    "## 5. GRADIENT BOOSTING LIBRARIES\n",
    "## ============================================================================\n",
    "\n",
    "## XGBoost\n",
    "# import xgboost as xgb\n",
    "# from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "## LightGBM\n",
    "# import lightgbm as lgb\n",
    "# from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "## Gradient Boosting Trees\n",
    "# from sklearn.ensemble import (\n",
    "#     GradientBoostingClassifier,\n",
    "#     GradientBoostingRegressor\n",
    "# )\n",
    "\n",
    "## CatBoost\n",
    "# from catboost import (\n",
    "#     CatBoostClassifier,\n",
    "#     CatBoostRegressor,\n",
    "#     Pool\n",
    "# )\n",
    "\n",
    "## ============================================================================\n",
    "## 6. NATURAL LANGUAGE PROCESSING\n",
    "## ============================================================================\n",
    "\n",
    "## NLTK\n",
    "# import nltk\n",
    "# from nltk.tokenize import (\n",
    "#     word_tokenize,\n",
    "#     sent_tokenize,\n",
    "#     RegexpTokenizer\n",
    "# )\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from nltk.sentiment.util import (\n",
    "#     mark_negation,\n",
    "#     negation_lookup,\n",
    "#     negation_words\n",
    "# )\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import (\n",
    "#     PorterStemmer,\n",
    "#     SnowballStemmer,\n",
    "#     WordNetLemmatizer\n",
    "# )\n",
    "# from nltk.tag import pos_tag\n",
    "# from nltk.chunk import ne_chunk\n",
    "# from nltk import ngrams\n",
    "# from nltk.util import ngrams, everygrams\n",
    "# from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "## spaCy\n",
    "# import spacy\n",
    "# from spacy import displacy\n",
    "# from spacy.lang.en import English\n",
    "# from spacy.tokens import Doc, Span, Token\n",
    "# from spacy.matcher import Matcher, PhraseMatcher\n",
    "# from spacy.pipeline import (\n",
    "#     EntityRuler,\n",
    "#     Sentencizer,\n",
    "#     MergeEntities,\n",
    "#     MergeNounChunks\n",
    "# )\n",
    "\n",
    "## TextBlob\n",
    "# from textblob import TextBlob\n",
    "# from textblob import Word, Blobber\n",
    "# from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "## Gensim\n",
    "# import gensim\n",
    "# from gensim import corpora, models\n",
    "# from gensim.models import Word2Vec, Doc2Vec, FastText, LdaModel\n",
    "# from gensim.similarities import MatrixSimilarity, SparseTermSimilarity\n",
    "# from gensim.parsing.preprocessing import (\n",
    "#     preprocess_string,\n",
    "#     strip_punctuation,\n",
    "#     strip_numeric,\n",
    "#     strip_multiple_whitespaces,\n",
    "#     strip_short,\n",
    "#     stem_text,\n",
    "#     lemmatize,\n",
    "#     stem_document,\n",
    "#     lemmatize_document\n",
    "# )\n",
    "\n",
    "## Transformers (Hugging Face)\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoModel,\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     AutoModelForQuestionAnswering,\n",
    "#     pipeline,\n",
    "#     Trainer,\n",
    "#     TrainingArguments\n",
    "# )\n",
    "# from transformers.data.processors.squad import SquadV2Processor\n",
    "# from transformers.data.processors.utils import InputFeatures\n",
    "# from transformers.data.metrics.squad_metrics import (\n",
    "#     squad_evaluate,\n",
    "#     squad_metrics\n",
    "# )\n",
    "# from transformers.data.data_collator import (\n",
    "#     DataCollatorWithPadding,\n",
    "#     DataCollatorForLanguageModeling\n",
    "# )\n",
    "\n",
    "## Sentence Transformers\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sentence_transformers import (\n",
    "#     models,\n",
    "#     losses,\n",
    "#     evaluation,\n",
    "#     util\n",
    "# )\n",
    "\n",
    "## ============================================================================\n",
    "## 7. COMPUTER VISION\n",
    "## ============================================================================\n",
    "\n",
    "## OpenCV\n",
    "# import cv2\n",
    "\n",
    "## PIL/Pillow\n",
    "# from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "## Albumentations (Data Augmentation)\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# from albumentations.augmentations.transforms import (\n",
    "#     HorizontalFlip,\n",
    "#     VerticalFlip, \n",
    "#     RandomRotate90,\n",
    "#     Transpose,\n",
    "#     ShiftScaleRotate,\n",
    "#     RandomBrightnessContrast,\n",
    "#     RandomGamma,\n",
    "#     RandomSunFlare,\n",
    "#     RandomRain,\n",
    "#     RandomFog,\n",
    "#     RandomSnow,\n",
    "#     RandomShadow,\n",
    "#     RandomScale,\n",
    "#     RandomCrop,\n",
    "#     RandomGridShuffle,\n",
    "#     RandomSizedCrop,\n",
    "#     RandomResizedCrop\n",
    "# )\n",
    "\n",
    "## ============================================================================\n",
    "## 8. TIME SERIES ANALYSIS\n",
    "## ============================================================================\n",
    "\n",
    "## Statsmodels\n",
    "# import statsmodels.api as sm\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "# from statsmodels.tsa.stattools import adfuller, kpss\n",
    "# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "## financial libraries\n",
    "# import ccxt\n",
    "# from fredapi import Fred\n",
    "# import yfinance as yf\n",
    "# import alpha_vantage as av\n",
    "\n",
    "## Prophet (Facebook Time Series)\n",
    "# from prophet import Prophet\n",
    "\n",
    "## ============================================================================\n",
    "## 9. HYPERPARAMETER OPTIMIZATION\n",
    "## ============================================================================\n",
    "\n",
    "## Optuna\n",
    "# import optuna\n",
    "# from optuna.samplers import (\n",
    "#     TPESampler,\n",
    "#     CmaEsSampler\n",
    "# )\n",
    "# from optuna.pruners import (\n",
    "#     MedianPruner,\n",
    "#     HyperbandPruner,\n",
    "#     SuccessiveHalvingPruner\n",
    "# )\n",
    "# from optuna.integration import (\n",
    "#     XGBoostPruningCallback,\n",
    "#     LightGBMPruningCallback\n",
    "# )\n",
    "\n",
    "## Hyperopt\n",
    "# from hyperopt import (\n",
    "#     fmin,\n",
    "#     tpe,\n",
    "#     hp,\n",
    "#     STATUS_OK,\n",
    "#     Trials\n",
    "# )\n",
    "# from hyperopt.pyll import scope\n",
    "\n",
    "## Ray Tune\n",
    "# from ray import tune\n",
    "# from ray.tune import CLIReporter\n",
    "# from ray.tune.schedulers import ASHAScheduler\n",
    "# from ray.tune.integration.optuna import OptunaSearch\n",
    "# from ray.tune.integration.keras import TuneReportCallback\n",
    "\n",
    "## ============================================================================\n",
    "## 10. MODEL INTERPRETATION & EXPLAINABILITY\n",
    "## ============================================================================\n",
    "\n",
    "## SHAP\n",
    "# import shap\n",
    "# from shap import (\n",
    "#     DeepExplainer,\n",
    "#     KernelExplainer,\n",
    "#     summary_plot,\n",
    "#     force_plot,\n",
    "#     interaction_values,\n",
    "#     summary_plot\n",
    "# )\n",
    "\n",
    "## LIME\n",
    "# from lime import lime_image, lime_text, lime_tabular\n",
    "\n",
    "## ELI5\n",
    "# import eli5\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "# from eli5.lime import TextExplainer\n",
    "# from eli5.explainers import explain_prediction, explain_weights\n",
    "# from eli5.formatters import format_as_text, format_as_html\n",
    "# from eli5.sklearn import explain_weights_sklearn\n",
    "# from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "## Permutation Importance\n",
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "## ============================================================================\n",
    "## 11. MLOPS & MODEL DEPLOYMENT\n",
    "## ============================================================================\n",
    "\n",
    "## MLflow\n",
    "# import mlflow\n",
    "# import mlflow.sklearn\n",
    "# import mlflow.keras\n",
    "# import mlflow.pytorch\n",
    "# import mlflow.xgboost\n",
    "# import mlflow.lightgbm\n",
    "# from mlflow.tracking import MlflowClient\n",
    "# from mlflow.models import infer_signature\n",
    "\n",
    "## Weights & Biases\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "# from wandb.integration.lightgbm import wandb_callback as lgb_wandb_callback\n",
    "# from wandb.integration.xgboost import wandb_callback as xgb_wandb_callback\n",
    "# from wandb.integration.pytorch import WandbLogger, WandbCallback\n",
    "# from wandb.integration.tensorflow import WandbCallback as tf_wandb_callback\n",
    "\n",
    "## TensorBoard\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "# from tensorboard.plugins.hparams import api as hparams\n",
    "\n",
    "## ============================================================================\n",
    "## 12. DATA LOADING & PROCESSING\n",
    "## ============================================================================\n",
    "\n",
    "## SQL Databases\n",
    "# import sqlite3\n",
    "# from sqlalchemy import create_engine\n",
    "# import pyodbc\n",
    "# import psycopg2\n",
    "# import pymongo\n",
    "# from pymongo import MongoClient, ASCENDING\n",
    "# from pymongo.errors import DuplicateKeyError, BulkWriteError\n",
    "# from sqlalchemy.orm import sessionmaker\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "# from sqlalchemy import Column, Integer, String, Float, DateTime, ForeignKey\n",
    "# from sqlalchemy.orm import relationship, backref\n",
    "# from sqlalchemy.exc import SQLAlchemyError, IntegrityError\n",
    "\n",
    "## File Formats\n",
    "# import h5py\n",
    "# import openpyxl\n",
    "# import xlrd\n",
    "# import pyarrow\n",
    "# import feather\n",
    "# import pyarrow.parquet as pq\n",
    "# import fastparquet\n",
    "# import jsonlines\n",
    "# import msgpack\n",
    "# import yaml\n",
    "# import toml\n",
    "# import csv\n",
    "# import json\n",
    "# import xml.etree.ElementTree as ET\n",
    "# import xmltodict\n",
    "\n",
    "## Web Scraping\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import scrapy\n",
    "# import selenium\n",
    "# from selenium import webdriver\n",
    "\n",
    "## API Handling\n",
    "# import requests\n",
    "# import urllib.request\n",
    "# import urllib.parse\n",
    "# from requests_oauthlib import OAuth1Session, OAuth2Session\n",
    "# from requests.exceptions import (\n",
    "#     HTTPError,\n",
    "#     Timeout,\n",
    "#     RequestException,\n",
    "#     ConnectionError,\n",
    "#     TooManyRedirects,\n",
    "#     URLRequired\n",
    "# )\n",
    "# from requests.auth import HTTPBasicAuth, HTTPDigestAuth, AuthBase\n",
    "# from requests_cache import CachedSession\n",
    "# from requests_toolbelt import MultipartEncoder, MultipartDecoder\n",
    "# from requests_toolbelt.adapters import host_header_ssl\n",
    "\n",
    "\n",
    "## ============================================================================\n",
    "## 13. STATISTICAL ANALYSIS\n",
    "## ============================================================================\n",
    "\n",
    "## SciPy\n",
    "# import scipy\n",
    "# from scipy import stats\n",
    "# from scipy.stats import (\n",
    "#     ttest_ind,\n",
    "#     chi2_contingency,\n",
    "#     pearsonr,\n",
    "#     spearmanr,\n",
    "#     kendalltau,\n",
    "#     shapiro,\n",
    "#     normaltest,\n",
    "#     kstest\n",
    "#     mannwhitneyu,\n",
    "#     wilcoxon,\n",
    "# )\n",
    "\n",
    "## ============================================================================\n",
    "## 14. ENVIRONMENT SETUP\n",
    "## ============================================================================\n",
    "\n",
    "## Set random seeds for reproducibility\n",
    "def set_random_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducible results\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # For TensorFlow/Keras\n",
    "    # tf.random.set_seed(seed)\n",
    "    \n",
    "    # For PyTorch\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## Call the function to set seeds\n",
    "set_random_seeds(42)\n",
    "\n",
    "## Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "# plt.style.use('ggplot')\n",
    "# plt.style.use('default')\n",
    "\n",
    "## Set seaborn style\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "## Set figure size\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"ðŸ”§ Environment configured for ML/DL projects\")\n",
    "print(\"ðŸ“Š Ready to start your machine learning journey!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cehkEcoOVi-x"
   },
   "source": [
    "## **Data Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Y1_cgyBDuRE8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "98d0b440-c620-447a-a03b-cb0b3452de4c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NFVaBmkW9wP"
   },
   "source": [
    "## **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxtPmsnWf2Pm"
   },
   "source": [
    "### **Retrieve Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-11T01:03:51.247296Z",
     "iopub.status.busy": "2025-06-11T01:03:51.246451Z",
     "iopub.status.idle": "2025-06-11T01:04:08.352456Z",
     "shell.execute_reply": "2025-06-11T01:04:08.351365Z",
     "shell.execute_reply.started": "2025-06-11T01:03:51.247265Z"
    },
    "id": "sUnIgdIkXHeJ",
    "outputId": "e8dc58df-1d07-4958-c32c-74a8eea2b8c4",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqFC7A0oNBG5"
   },
   "source": [
    "### **Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUrMuK-oOPoA"
   },
   "source": [
    "## **Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-11T01:05:55.267679Z",
     "iopub.status.busy": "2025-06-11T01:05:55.267307Z",
     "iopub.status.idle": "2025-06-11T01:05:55.353666Z",
     "shell.execute_reply": "2025-06-11T01:05:55.352671Z",
     "shell.execute_reply.started": "2025-06-11T01:05:55.267652Z"
    },
    "id": "DPEruixbOY3q",
    "outputId": "daf68e18-4968-4323-af12-98c3e5b0636e",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:06:00.630342Z",
     "iopub.status.busy": "2025-06-11T01:06:00.629737Z",
     "iopub.status.idle": "2025-06-11T01:06:00.647166Z",
     "shell.execute_reply": "2025-06-11T01:06:00.646440Z",
     "shell.execute_reply.started": "2025-06-11T01:06:00.630314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARwKLaaxP0u4"
   },
   "source": [
    "## **Features Engineering and Features Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJHjAqcnPHeC"
   },
   "source": [
    "### **Set Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_LDzv77Qv4V"
   },
   "source": [
    "### **Features Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Features Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUDCKA6tStUR"
   },
   "source": [
    "## **Data Preparation and Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:06:56.381937Z",
     "iopub.status.busy": "2025-06-11T01:06:56.381224Z",
     "iopub.status.idle": "2025-06-11T01:07:08.934164Z",
     "shell.execute_reply": "2025-06-11T01:07:08.933504Z",
     "shell.execute_reply.started": "2025-06-11T01:06:56.381914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:07:17.171999Z",
     "iopub.status.busy": "2025-06-11T01:07:17.171693Z",
     "iopub.status.idle": "2025-06-11T01:07:17.335324Z",
     "shell.execute_reply": "2025-06-11T01:07:17.334508Z",
     "shell.execute_reply.started": "2025-06-11T01:07:17.171978Z"
    },
    "id": "HWaDtPNUSx1Z",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deep Learning Algorithm Class (if needed)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:07:24.935778Z",
     "iopub.status.busy": "2025-06-11T01:07:24.935465Z",
     "iopub.status.idle": "2025-06-11T01:07:24.944372Z",
     "shell.execute_reply": "2025-06-11T01:07:24.943568Z",
     "shell.execute_reply.started": "2025-06-11T01:07:24.935758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Your Machine Learning Algorithm (if needed)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup Multi-GPU (only for 2 or more GPUs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:07:33.358747Z",
     "iopub.status.busy": "2025-06-11T01:07:33.358118Z",
     "iopub.status.idle": "2025-06-11T01:07:33.372837Z",
     "shell.execute_reply": "2025-06-11T01:07:33.371982Z",
     "shell.execute_reply.started": "2025-06-11T01:07:33.358722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShFPQUJxxZYT"
   },
   "source": [
    "## **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T19:02:55.486090Z",
     "iopub.status.busy": "2025-06-10T19:02:55.485810Z",
     "iopub.status.idle": "2025-06-10T20:20:57.438489Z",
     "shell.execute_reply": "2025-06-10T20:20:57.437767Z",
     "shell.execute_reply.started": "2025-06-10T19:02:55.486063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1HZZRG9Uvp1"
   },
   "source": [
    "## **Training Model with Best Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:09:10.623433Z",
     "iopub.status.busy": "2025-06-11T01:09:10.623125Z",
     "iopub.status.idle": "2025-06-11T01:30:06.681448Z",
     "shell.execute_reply": "2025-06-11T01:30:06.680466Z",
     "shell.execute_reply.started": "2025-06-11T01:09:10.623411Z"
    },
    "id": "ti0fJ25aU5dP",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwE4lW6HUvaZ"
   },
   "source": [
    "## **Model Evaluation and Backtesting (if needed)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:59:04.033108Z",
     "iopub.status.busy": "2025-06-11T01:59:04.032371Z",
     "iopub.status.idle": "2025-06-11T02:00:31.224894Z",
     "shell.execute_reply": "2025-06-11T02:00:31.224039Z",
     "shell.execute_reply.started": "2025-06-11T01:59:04.033082Z"
    },
    "id": "lL6Z3SavVUPq",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIN6g2ZUV1bu"
   },
   "source": [
    "## **Model Persistance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "r96dThRHUzOE",
    "cehkEcoOVi-x",
    "4NFVaBmkW9wP",
    "RxtPmsnWf2Pm",
    "wUZE7aoBN-iB",
    "hpMpHGOGOGHh",
    "kUrMuK-oOPoA"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
